{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import requests \n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from os import makedirs\n",
    "from os.path import exists\n",
    "import glob\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import spacy\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import joblib\n",
    "import pickle"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/jinglin/anaconda3/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "/Users/jinglin/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1 : \n",
    "- Go to the page listing all songs of your favourite artist on lyrics.com.\n",
    "- Copy the URL\n",
    "- Use the requests module to download that page\n",
    "- Examine the HTML code and look for links to songs\n",
    "- Extract all links using Regular Expressions or BeautifulSoup\n",
    "- Use the requests module to download song page (with not redundent song names) containing lyrics\n",
    "- Save in artist folder text files, one songe page per text file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# function : get song urls\n",
    "\n",
    "def get_song_urls(artist, artist_url):\n",
    "    ''' input : artist str, artist_url str\n",
    "        return : song_urls list '''\n",
    "\n",
    "    request_artist_url = requests.get(artist_url)\n",
    "    prefix = 'https://www.lyrics.com'\n",
    "    lyric_soup = BeautifulSoup(request_artist_url.text, 'html.parser') \n",
    "    song_urls = []\n",
    "    for album in lyric_soup.find('div', class_=\"tdata-ext\").find_all('tbody'):\n",
    "        song_urls += [prefix + x['href'] for x in album.find_all('a')] \n",
    "    return song_urls\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# function : get song urls drop redundent ones with exact song name match\n",
    "\n",
    "def get_song_urls_dr(artist, artist_url):\n",
    "    ''' input : artist str, artist_url str\n",
    "        return : song_urls list '''\n",
    "\n",
    "    request_artist_url = requests.get(artist_url)\n",
    "    prefix = 'https://www.lyrics.com'\n",
    "    lyric_soup = BeautifulSoup(request_artist_url.text, 'html.parser') \n",
    "    song_urls = []\n",
    "    song_titles = []\n",
    "    for album in lyric_soup.find('div', class_=\"tdata-ext\").find_all('tbody'):\n",
    "        # append url if not duplicate song\n",
    "        for i_title in np.arange(len(album.find_all('a'))):\n",
    "            title = album.find_all('a')[i_title].text\n",
    "            if title not in song_titles:\n",
    "                song_titles += [title]\n",
    "                song_urls += [prefix + x['href'] for x in album.find_all('a')] \n",
    "    return song_urls\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# function : get song urls drop redundent ones with fuzzywuzzy 90% match\n",
    "\n",
    "def get_song_urls_drf(artist, artist_url):\n",
    "    ''' input : artist str, artist_url str\n",
    "        return : song_urls list '''\n",
    "\n",
    "    request_artist_url = requests.get(artist_url)\n",
    "    prefix = 'https://www.lyrics.com'\n",
    "    lyric_soup = BeautifulSoup(request_artist_url.text, 'html.parser') \n",
    "    song_urls = []\n",
    "    song_titles = ['']\n",
    "    for album in lyric_soup.find('div', class_=\"tdata-ext\").find_all('tbody'):\n",
    "        # append url if not duplicate song\n",
    "        for i_title in np.arange(len(album.find_all('a'))):\n",
    "            title = album.find_all('a')[i_title].text\n",
    "            if title not in song_titles: # first layer filter to save time\n",
    "                best_match = 0\n",
    "                best_match_title = ''\n",
    "                for given_title in song_titles:\n",
    "                    score = fuzz.ratio(title.lower(), given_title.lower())\n",
    "                    # from fuzz import process\n",
    "                    # process.dedupe\n",
    "                    # threshold !!!\n",
    "                    if  score > best_match:\n",
    "                        best_match = score\n",
    "                        best_match_title = given_title\n",
    "                if best_match < 90:      \n",
    "                    song_titles += [title]\n",
    "                    x = album.find_all('a')[i_title]\n",
    "                    song_urls += [prefix + x['href']]\n",
    "                else:\n",
    "                    print(f'fuzzy title match : {title} vs {best_match_title}') # for testing removed titles\n",
    "            #else:\n",
    "                #print(f'exact title match : {title}')\n",
    "    return song_urls\n",
    "\n",
    "#a = get_song_urls_drf('simon_garfunkel', 'https://www.lyrics.com/artist/Simon-%26-Garfunkel/5431') "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# function : save song pages to txt files\n",
    "\n",
    "def save_song_page(artist, song_urls, path):\n",
    "    ''' input : artist str\n",
    "                song_url list containing song urls of an artist\n",
    "                path str\n",
    "        do : save song page to txt files with filename start with artist to path artist folder'''\n",
    "\n",
    "    if not exists(f'{path}/{artist}'):\n",
    "        makedirs(f'{path}/{artist}')\n",
    "    for i in tqdm(np.arange(len(song_urls))):\n",
    "        time.sleep(0.5)\n",
    "        one_song_request = requests.get(song_urls[i])\n",
    "        open(f'{path}/{artist}/{artist}_song_{i}.txt', 'w').write(one_song_request.text)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# function : strip song page and save lyrics to txt file\n",
    "\n",
    "def save_lyric_files(artist, path):\n",
    "    ''' input : artist str\n",
    "                path str\n",
    "        do : save striped lyric to txt files with filename start with artist to path artist folder'''\n",
    "\n",
    "    path_filenames = (path + f'/{artist}/' + f'{artist}_song_*.txt')\n",
    "    song_files = [f for f in sorted(glob.glob(path_filenames))]\n",
    "    for i in tqdm(np.arange(len(song_files))):\n",
    "        # save the text in the song txt files into variable text\n",
    "        text = open(song_files[i], 'r').read()\n",
    "        lyric_soup = BeautifulSoup(text, 'html.parser') # text here is the content of txt file, and is equivalent to requests.text\n",
    "        # strip\n",
    "        if lyric_soup.find('pre') is not None:\n",
    "            open(f'./{artist}/{artist}_lyric_{i}.txt', 'a').write(lyric_soup.find('pre').text)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# run all steps for two artists\n",
    "\n",
    "artists = ['simon_garfunkel', 'queen']\n",
    "artist_urls = ['https://www.lyrics.com/artist/Simon-%26-Garfunkel/5431', 'https://www.lyrics.com/artist/Queen/5205']\n",
    "path = '/Users/jinglin/Documents/spiced_projects/unsupervised-lemon-student-code/week_04/project'\n",
    "\n",
    "for n in np.arange(len(artists)): \n",
    "    artist = artists[n]\n",
    "    artist_url = artist_urls[n]\n",
    "\n",
    "    # get song urls\n",
    "    song_urls = get_song_urls_drf(artist, artist_url)\n",
    "    print(len(song_urls))\n",
    "\n",
    "    # save song page \n",
    "    save_song_page(artist, song_urls, path)\n",
    "\n",
    "    # save striped lyric to txt files\n",
    "    save_lyric_files(artist, path)\n",
    "\n",
    "# simon_garfunkel : no drop 1875, drop exact match 283, drop fuzzywuzzy match 204 remained\n",
    "# queen : before droping over 3800, after 578\n",
    "# some of 204 and 578, the html lyric body pre is none and skipped -> in the end two artist together 738 lyrics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## step 2: \n",
    "- Create corpus and labels from lyrics txt files of both artists\n",
    "    - X : corpus, vector, each value = string one song\n",
    "    - y : labels, vector, each value = one artist name\n",
    "- Train test split ! otherwise information leakage\n",
    "- Deal with class imbalance with training data  \n",
    "    - (this step do not belong to pipeline, do not do for test)\n",
    "- Feature Engineer Pipeline \n",
    "    - Cleaning: re\n",
    "    - vectorization : \n",
    "        - Countvectorizer, TfIdfVectorizer, or doc2vec vectorizer\n",
    "        - can filter with english stopwords, maximal document frequency, minimal document frequency (appearence over all songs)\n",
    "- Machine Learning, Train, CV, Hyperparameter tunning cycle \n",
    "- Test check\n",
    "- Predict an external x "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# function create corpus and labels\n",
    "\n",
    "def get_corpus_labels(artists, path):\n",
    "    '''input : artists str-list \n",
    "               path str path to the artist folders containting lyrics txt files\n",
    "       return : corpus list each row a str with lyrics of one song\n",
    "                labels list each row a str of artist name'''\n",
    "    corpus = []\n",
    "    labels = []\n",
    "    labels_n = []\n",
    "    for i_artist in np.arange(len(artists)):\n",
    "        artist = artists[i_artist] \n",
    "        path_filenames = (path + f'/{artist}/' + f'{artist}_lyric_*.txt')\n",
    "        song_files = [f for f in sorted(glob.glob(path_filenames))]\n",
    "        for i in range(len(song_files)):\n",
    "            text = open(song_files[i], 'r').read()\n",
    "            corpus.append(text)\n",
    "            labels.append(artist) # string labels\n",
    "            labels_n.append(i_artist) # 0,1 labels\n",
    "    return corpus, labels, labels_n\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "artists = ['simon_garfunkel', 'queen']\n",
    "path = '/Users/jinglin/Documents/spiced_projects/unsupervised-lemon-student-code/week_04/project'\n",
    "(corpus, labels, labels_n) = get_corpus_labels(artists, path)\n",
    "len(corpus), len(labels), type(corpus), type(labels), labels[0], labels_n[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(738, 738, list, list, 'simon_garfunkel', 0)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# feature and label/target value\n",
    "X = pd.DataFrame({'lyric' : corpus})\n",
    "y = pd.DataFrame({'artist' : labels})\n",
    "\n",
    "# train test split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "print(Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape)\n",
    "print(ytrain.describe())\n",
    "print(ytest.describe())\n",
    "\n",
    "# class imbalance "
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n    "
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n    "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(553, 1) (185, 1) (553, 1) (185, 1)\n",
      "       artist\n",
      "count     553\n",
      "unique      2\n",
      "top     queen\n",
      "freq      402\n",
      "       artist\n",
      "count     185\n",
      "unique      2\n",
      "top     queen\n",
      "freq      142\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# deal with imbalance \n",
    "# original Xtrain simon_garfunkel 151, queen 402; increase simon_garfunkel to 250\n",
    "ros = RandomOverSampler(sampling_strategy={'simon_garfunkel' : 250}, random_state=0)\n",
    "X_ros, y_ros = ros.fit_resample(Xtrain, ytrain)\n",
    "print(X_ros.shape, y_ros.shape)\n",
    "print(y_ros.describe()) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(652, 1) (652, 1)\n",
      "       artist\n",
      "count     652\n",
      "unique      2\n",
      "top     queen\n",
      "freq      402\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# cleaning : need to add e.g. stemming \n",
    "def lyric_cleaning(X):\n",
    "    ''' text cleaning : input can be str, list of sring, or pandas Series '''\n",
    "    if isinstance(X, str): # e.g. \"hello darkness my old friend\"\n",
    "        x_clean = re.sub(r'\\n', ' ', X.lower())\n",
    "    elif isinstance(X, list): # e.g. Xtrain['lyric'].tolist()\n",
    "        x_clean = [x.replace('\\n', ' ') for x in X]\n",
    "    else: # e.g. Xtrain['lyric']\n",
    "        x_clean = [re.sub(r'\\n', ' ', x.lower()) for x in X]\n",
    "    return x_clean"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# try feature englineering - machine learning - pipeline\n",
    "fe_pipe = make_pipeline(\n",
    "    FunctionTransformer(lyric_cleaning), \n",
    "    TfidfVectorizer(stop_words='english', max_df=0.8, min_df=0.01, ngram_range=(1,2))\n",
    ")\n",
    "\n",
    "fe_ml_pipe = Pipeline([\n",
    "    ('fe', fe_pipe),\n",
    "    ('logreg', LogisticRegression(random_state=0))\n",
    "])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "fe_ml_pipe.fit(Xtrain['lyric'].tolist(), ytrain['artist'].tolist())\n",
    "train_score = fe_ml_pipe.score(Xtrain['lyric'].tolist(), np.ravel(ytrain))\n",
    "test_score = fe_ml_pipe.score(Xtest['lyric'].tolist(), np.ravel(ytest))\n",
    "print(train_score, test_score)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9240506329113924 0.9135135135135135\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# save and load model so that can be used without have to train again\n",
    "joblib.dump(fe_ml_pipe, 'lyric.mod') \n",
    "model = joblib.load('lyric.mod')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "joblib.dump(fe_ml_pipe, 'lyric.pkl') \n",
    "model_pp = joblib.load('lyric.pkl') "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "pickle.dump(fe_ml_pipe, open('lyric.sav', 'wb'))\n",
    "model_p = pickle.load(open('lyric.sav', 'rb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# try an external example \n",
    "userinput = \"hello darkness my old friend \"\n",
    "print(model_p.predict([userinput]))\n",
    "userinput = \"hello darkness my old friend I've come to talk to you again\"\n",
    "print(model_pp.predict([userinput]))\n",
    "userinput = \"And here's to you, \"\n",
    "print(model.predict([userinput]))\n",
    "userinput = \"And here's to you, Mrs. Robinson \"\n",
    "print(model.predict([userinput]))\n",
    "userinput = \"And here's to you, Mrs. Robinson Jesus loves you more than you will know Whoa, whoa, whoa\"\n",
    "print(model.predict([userinput]))\n",
    "userinput = \"And here's to you, Mrs. Robinson Jesus loves you more than you will know Whoa, whoa, whoaGod bless you, please, Mrs. Robinson Heaven holds a place for those who pray Hey, hey, hey Hey, hey, hey\"\n",
    "print(model.predict([userinput]))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['queen']\n",
      "['queen']\n",
      "['queen']\n",
      "['simon_garfunkel']\n",
      "['queen']\n",
      "['simon_garfunkel']\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "e693f81ce65b2d86a8ccaa4486a6c3213eba987e9092bbfb488d6be295156d0f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}